---
updateTime: '2025-12-21 18:26'
tags: AI
---
> 记录一次利用**Transformer**实现的文本+文字双模态情感识别实验
## 一、数据集介绍
MVSA（Multi-View Social Analysis）数据集是由Yuan、You、Mcdonough和Luo等人于2017年创建并发布的一项多模态社交媒体情感分析数据集。

1.1 数据构成：
MVSA包含图像-文本对样本。其具体数量分布如下：
- 训练集：3,408个样本（3,408张图像 + 3,408条对应文本）。
- 验证集：730个样本（730张图像 + 730条对应文本）。
- 测试集：731个样本（731张图像 + 731条对应文本）。

1.2 情感标签分布：
基于对样本的情感标注，数据集中情感标签的整体比例如下：
- 积极 (Positive)：约59.5% (2,682个样本)。
- 消极 (Negative)：约30.1% (1,358个样本)。
- 中性 (Neutral)：约10.4% (470个样本)。
> 该分布表明情感标签存在一定的**类别不平衡性**，积极情绪占主导。

<table><thead><tr><th>数据集分布</th><th>数据集大小尺寸</th></tr></thead><tbody><tr><td><img src="/minio/weblog/a79350534c564c41a68a4a7e1bb57c65.png" alt="数据集分布"></td><td><img src="/minio/weblog/93ac459cab994544aaa1c7ae9990d405.png" alt="数据集大小尺寸"></td></tr></tbody></table>


![](/minio/weblog/bc6af582e1054bcba48ef11b6c3f74e7.png)
### 构建数据集
```py

# 自定义数据集
class MVSA_Dataset(Dataset):
    def __init__(self, text_data, image_data, labels, tokenizer, transform):
        self.text_data = text_data
        self.image_data = image_data
        self.labels = labels
        self.tokenizer = tokenizer # bert的分词器
        self.transform = transform

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        text = self.text_data[idx]
        image = Image.open(self.image_data[idx]).convert('RGB')
        label = self.labels[idx]
        
        # 文本处理
        text_encoded = self.tokenizer(
            text, 
            max_length=64, 
            padding='max_length', 
            truncation=True, 
            return_tensors='pt'
        )
        
        # 图像处理
        image = self.transform(image) # 归一化
        
        return {
            'input_ids': text_encoded['input_ids'].squeeze(0),
            'attention_mask': text_encoded['attention_mask'].squeeze(0),
            'image': image,
            'label': torch.tensor(label, dtype=torch.long)
        }


```
## 二、方法介绍
### 双流特征提取
> 采用**ResNet-50提取图像特征**（输出维度256），**BERT-base编码文本特征**（输出维度768）
![](/minio/weblog/c652ab81a68f4c9290ed45b3077beb98.png)
> ![](/minio/weblog/dafe9eafb2c5466795c69d17eee1fde3.png)
> 视觉引导注意力机制：以**图像特征为查询向量**（Query），对齐**文本特征为键值对**（Key-Value），构建多头交叉注意力模块（VG-MHCA）
>
![](/minio/weblog/851ef9811e6a4bdfa4832f5e752588ab.png)

### 模态特征对齐
- 视觉特征提取：采用预训练的ResNet-50骨干网络提取图像表示。输入图像经标准化处理后（尺寸224×224，RGB三通道），输出特征维度为v∈R 256
- 文本特征提取：基于BERT-base模型处理输入文本，输出768维语义向量t∈R 768
- 维度对齐：通过**全连接层**将文本特征投影至视觉特征空间：
![](/minio/weblog/296da7bb94ff48e39fcd9a9ecce6fcbc.png)

其中Wt∈R256×768为可学习权重矩阵，bt​为偏置项
### 构建模型
```py
# 多模态模型
class MultimodalModel(nn.Module):
    def __init__(self, config):
        super(MultimodalModel, self).__init__()
        self.config = config
        
        # 文本分支 (BERT)
        self.text_model = BertModel.from_pretrained(config.text_model_name)
        self.text_fc = nn.Sequential(
            nn.Linear(768, config.hidden_size),
            nn.ReLU(),
            nn.Dropout(config.dropout_prob)
        )
        
        # 图像分支 (ResNet)
        self.image_model = resnet50(pretrained=True)
        self.image_model.fc = nn.Identity()  # 移除原始分类层
        self.image_fc = nn.Sequential(
            nn.Linear(2048, config.hidden_size),
            nn.ReLU(),
            nn.Dropout(config.dropout_prob)
        )
        
        # 交叉多头注意力 (CMA)
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=config.hidden_size,
            num_heads=config.num_heads,
            dropout=config.dropout_prob
        )
        self.layer_norm = nn.LayerNorm(config.hidden_size)
        
        # 分类头
        self.classifier = nn.Linear(config.hidden_size, config.num_classes)

    def forward(self, input_ids, attention_mask, image):
        # 文本特征提取
        text_outputs = self.text_model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        text_features = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token
        text_features = self.text_fc(text_features)  # (batch_size, hidden_size)
        
        # 图像特征提取
        image_features = self.image_model(image)  # (batch_size, 2048)
        image_features = self.image_fc(image_features)  # (batch_size, hidden_size)
        
        # 交叉注意力 (文本作为Query, 图像作为Key/Value)
        text_features = text_features.unsqueeze(0)  # (1, batch_size, hidden_size)
        image_features = image_features.unsqueeze(0)  # (1, batch_size, hidden_size)
        
        attn_output, _ = self.cross_attention(
            query=text_features,
            key=image_features,
            value=image_features
        )
        attn_output = self.layer_norm(attn_output.squeeze(0))
        
        # 分类
        logits = self.classifier(attn_output)
        return logits
```
## 交叉注意力过程
![](/minio/weblog/3b88417579de45de9e5b2a7bf6103fa0.png)
> 为什么要用图像作为q，文本作为k和v
在情感分析任务中：
> - **视觉特征**：通常包含更直接的情感信号（表情、肢体语言等）
> - **文本特征**：提供语义解释和语境补充
> 为什么**视觉作为主导查询**？人脑处理：看到笑脸 → 寻找解释性文字 → 确认情感
>
> 举例：<br>
> **输入**:<br>
> image = "**皱眉人像**" <br>
> text = "**虽然产品价格高，但质量不错**"<br>
>  **处理流程**:<br>
> 1. 视觉特征(Q): 检测到"皱眉"→负面情感信号<br>
> 2. 文本特征(K/V): <br>
>     Key: 价格高(负面), 质量好(正面)<br>
> 3. 注意力: <br>
>    视觉Q高权重**关注"价格高"文本**<br>
> 4. 输出: **整体负面情感**<br>
> 如果文本作为Q: "价格高"可能错误关注图像中的产品细节

## 评价指标
在本次实验中，为全面评估不同模型在人脸表情分类任务中的性能，我们选择以下评价指标进行分析：准确率（Accuracy）、F1分数（F1 Score）、损失值（Loss）以及混淆矩阵（Confusion Matrix）。这些指标能够从不同维度反映模型的分类效果和对不同类别的区分能力。

<table><thead><tr><th>F1 Score</th><th>训练Loss</th></tr></thead><tbody><tr><td><img src="/minio/weblog/ca2e9690ccac434e8d26a8a1de063864.png" alt="F1 Score"></td><td><img src="/minio/weblog/1b27ce3900eb4e2bb3f57b2e5eb113bf.png" alt="训练Loss"></td></tr></tbody></table>

> **混淆矩阵**
> ![](/minio/weblog/8d641382eb6d42cb8388f302919abc5f.png)

## 抽样检测
![](/minio/weblog/ae89d1dd09ea403da96b17d93fa82ac3.png)
> **问题分析**
> - **中性样本识别失效根源**
>   - 数据层面：中性样本占比不足10%，且多含复杂语
>   - 样本1文本: "parents shunned jumper for leaving hasidic judaism" 
>   - 样本6文本: "cooper vs hargreaves top screen ball not thrown but notice no9s suddenness ability"
> - **文本包含混合情感信号**："shunned"（负面）但描述的是宗教信仰自由（中性/正面）
>   - 体育术语中的"ability"（能力）可能被误读为积极信号
>   - "forsaken world o mmorpg com um sistema de cria oinovador"
>   - "inovador"被解释为innovative（创新的）强积极信号
